{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 知识准备\n",
    "\n",
    "* NumPy\n",
    "* k-近邻算法原理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-近邻算法原理回顾\n",
    "\n",
    "**找k个距离x最近的样本，以此预测x的类别。**\n",
    "\n",
    "1. 确定超参k\n",
    "    * 交叉验证 D->m份  平均错误率  标准差\n",
    "2. 确定距离度量\n",
    "    * $L_p = \\left[\\displaystyle\\sum_{k=1}^{d}|x_{ik} - x_{jk}|^p\\right]^{\\frac{1}{p}}$ \n",
    "    * $p = 1$ 曼哈顿距离 $L_1 = \\displaystyle\\sum_{k=1}^{d}|x_{ik} - x_{jk}|$\n",
    "    * $p = 2$ 欧氏距离 $L_2 = \\left[\\displaystyle\\sum_{k=1}^{d}|x_{ik} - x_{jk}|^2\\right]^{\\frac{1}{2}}$\n",
    "    * $p = \\infty$ 切氏距离 $L_\\infty = \\mathop{argmax}\\limits_{1\\leq k \\leq d}|x_{ik} - x_{jk}|$\n",
    "3. 数据预处理 - 数据标准化\n",
    "    * z-score规范化 $x'_i = \\frac{x_i - \\mu}{\\sigma}$\n",
    "    * 最大-最小规范化 $x'_i = \\frac{x_i - min}{max - min}$\n",
    "4. 计算距离，找到x的k个近邻样本\n",
    "5. 预测样本类别\n",
    "    * 多数选举 最近邻\n",
    "    * 加权重\n",
    "\n",
    "\n",
    "# 案例简介\n",
    "## 背景\n",
    "海伦一直使用在线约会网站寻找适合自己的约会对象，她发现曾交往过三种类型的人：\n",
    "* 不喜欢的人\n",
    "* 魅力一般的人\n",
    "* 极具魅力的人\n",
    "\n",
    "她觉得可以在周一到周五约会那些魅力一般的人，而周末则更喜欢与那些极具魅力的人为伴。\n",
    "\n",
    "海伦希望分类软件可以更好地帮助她将匹配对象划分到确切的分类中。\n",
    "## 数据描述\n",
    "海伦收集的1000条约会数据，包含三个特征分别为\n",
    "* 每年获得的飞行常客里程数\n",
    "* 玩视频游戏所消耗时间百分比\n",
    "* 每周消费的冰淇淋公升数\n",
    "\n",
    "标签为\n",
    "* 不喜欢\n",
    "* 有些喜欢\n",
    "* 非常喜欢\n",
    "\n",
    "# 实现步骤\n",
    "\n",
    "1. 准备训练集D\n",
    "\n",
    "2. 对训练集D的属性进⾏预处理 **最大-最小规范化**\n",
    "\n",
    "3. 训练集D内找到预处理的样本x的前k个近邻 **欧氏距离**\n",
    "\n",
    "4. 结合**多数选举**的分类规则，对x的类别y进⾏预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 准备训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file2matrix(filename):\n",
    "    \"\"\"\n",
    "    函数说明：打开解析文件，对数据进行分类，\n",
    "        1代表不喜欢，2代表魅力一般，3代表极具魅力\n",
    "\n",
    "    Parameters:\n",
    "        filename - 文件名\n",
    "\n",
    "    Returns:\n",
    "        returnMat - 特征矩阵\n",
    "        classLabelVector - 分类label向量\n",
    "    \"\"\"\n",
    "    # 打开文件\n",
    "    with open(filename, 'r') as fr:\n",
    "        # 读取文件所有内容\n",
    "        arrayOflines = fr.readlines()\n",
    "   \n",
    "    # 得到文件行数\n",
    "    numberOfLines = len(arrayOflines)\n",
    "    # 返回的NumPy矩阵numberOfLines行，3列\n",
    "    returnMat = np.zeros((numberOfLines, 3))\n",
    "    # 创建分类标签向量\n",
    "    classLabelVector = []\n",
    "    # 行的索引值\n",
    "    index = 0\n",
    "    # 读取每一行\n",
    "    for line in arrayOflines:\n",
    "        # 去掉每一行首尾的空白符，例如'\\n','\\r','\\t',' '\n",
    "        line = line.strip()\n",
    "        # 将每一行内容根据'\\t'符进行切片,本例中一共有4列\n",
    "        listFromLine = line.split('\\t')\n",
    "        # 加入字符串转float的代码\n",
    "        # 将数据的前3列进行提取保存在returnMat特征矩阵中\n",
    "        # returnMat[index,:] = listFromLine[0:3]\n",
    "        returnMat[index,:] = [float(i) for i in listFromLine[:-1]]\n",
    "        # 根据文本内容进行分类1：不喜欢；2：一般；3：喜欢\n",
    "        if listFromLine[-1] == 'didntLike':\n",
    "            classLabelVector.append(1)\n",
    "        elif listFromLine[-1] == 'smallDoses':\n",
    "            classLabelVector.append(2)\n",
    "        elif listFromLine[-1] == 'largeDoses':\n",
    "            classLabelVector.append(3)\n",
    "        index += 1\n",
    "    # 返回标签列向量以及特征矩阵\n",
    "    return returnMat, np.array(classLabelVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 对训练集D的属性进行预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoNorm(dataSet):\n",
    "    \"\"\"\n",
    "    函数说明：对数据进行归一化(或称为，规范化)。\n",
    "    使用最小最大规范化，将数据值映射到[0,1]之间。\n",
    "    规范化后的x = (x-min)/(max-min)\n",
    "\n",
    "    Parameters:\n",
    "        dataSet - 特征矩阵\n",
    "\n",
    "    Returns:\n",
    "        normDataSet - 归一化后的特征矩阵\n",
    "        ranges - 数据范围\n",
    "        minVals - 数据最小值\n",
    "    \"\"\"\n",
    "    # 获取矩阵每一列的最小值\n",
    "    minVals = dataSet.min(0)\n",
    "    # 获取矩阵每一列的最大值\n",
    "    maxVals = dataSet.max(0)\n",
    "    # 最大值和最小值的范围\n",
    "    ranges = maxVals - minVals\n",
    "    # shape(dataSet)返回dataSet的矩阵行列数\n",
    "    normDataSet = np.zeros(np.shape(dataSet))\n",
    "    # numpy函数shape[0]返回dataSet的行数\n",
    "    m = dataSet.shape[0]\n",
    "    # 原始值减去最小值（x-xmin）  \n",
    "    # np.tile()将minVals复制成m行\n",
    "    normDataSet = dataSet - np.tile(minVals, (m, 1))\n",
    "    # 差值除以最大值和最小值的差值（x-xmin）/（xmax-xmin）\n",
    "    normDataSet = normDataSet / np.tile(ranges, (m, 1))\n",
    "    # 归一化数据结果，数据范围，最小值\n",
    "    return normDataSet, ranges, minVals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 寻找样本x的前k个近邻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify0(inX, dataSet, labels, k):\n",
    "    \"\"\"\n",
    "    函数说明：kNN算法，分类器\n",
    "\n",
    "    Parameters:\n",
    "        inX - 用于分类的数据（测试集）\n",
    "        dataSet - 用于训练的数据（训练集）（n*3矩阵）\n",
    "        labels - 分类标准（n*1维列向量）\n",
    "        k - kNN算法参数，选择距离最小的k个点\n",
    "\n",
    "    Returns:\n",
    "        sortedClassCount[0][0] - 分类结果\n",
    "    \"\"\"\n",
    "    # 返回dataSet的样本数量\n",
    "    dataSetSize = dataSet.shape[0]\n",
    "    # 将inX重复dataSetSize次并排成一列\n",
    "    xTileMat = np.tile(inX, (dataSetSize, 1)) \n",
    "    \n",
    "    # 特征相减后，对每个元素取平方\n",
    "    sqDiffMat = (xTileMat- dataSet)**2\n",
    "    # sum()所有元素相加，sum(0)列相加，sum(1)行相加\n",
    "    sqDistances = sqDiffMat.sum(axis=1)\n",
    "    # 开方，计算出距离\n",
    "    distances = sqDistances**0.5\n",
    "    \n",
    "    # argsort函数返回的是distances值从小到大的--索引值\n",
    "    sortedDistIndicies = distances.argsort()\n",
    "    # 定义一个记录类别次数的字典\n",
    "    classCount = {}\n",
    "    # 选择距离最小的k个点\n",
    "    for i in range(k):\n",
    "        # 取出前k个元素的类别\n",
    "        voteIlabel = labels[sortedDistIndicies[i]]\n",
    "        # 字典的get()方法，返回指定键的值，\n",
    "        # 如果值不在字典中返回0。\n",
    "        # 计算类别次数\n",
    "        classCount[voteIlabel] = classCount.get(voteIlabel, 0) + 1\n",
    "    # reverse降序排序字典\n",
    "    sortedClassCount = sorted(classCount.items(),\n",
    "                              key = operator.itemgetter(1), \n",
    "                              reverse = True)\n",
    "    # 返回次数最多的类别，即所要分类的类别\n",
    "    return sortedClassCount[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 结合多数选举规则，预测x的类别y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datingClassTest():\n",
    "    \"\"\"\n",
    "    函数说明：分类器测试函数\n",
    "\n",
    "    Parameters:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # 1.准备数据-从文本文件中解析数据、归一化\n",
    "    # 打开文件名\n",
    "    filename = \"datingTestSet.txt\"\n",
    "    # 将返回的特征矩阵和分类向量分别存储到\n",
    "    # datingDataMat和datingLabels中\n",
    "    datingDataMat, datingLabels = file2matrix(filename)\n",
    "    # 数据规范化(最小最大规范化)，返回规范化数据结果，数据范围，最小值\n",
    "    normMat, ranges, minVals = autoNorm(datingDataMat)\n",
    "    # 2.数据集划分\n",
    "    # 获取normMat的行数\n",
    "    m = normMat.shape[0]\n",
    "    # np.random.permutation获得打乱的索引值\n",
    "    idxs = np.random.permutation(m)\n",
    "    # 利用打乱的索引打乱数据\n",
    "    normMat = normMat[idxs]\n",
    "    datingLabels = datingLabels[idxs]\n",
    "    # 取所有数据的10%用于测试\n",
    "    hoRatio = 0.10\n",
    "    # 10%的测试数据的个数\n",
    "    numTestVecs = int(m * hoRatio)\n",
    "    trainX = normMat[numTestVecs:m,:]\n",
    "    trainY = datingLabels[numTestVecs:m]\n",
    "    testX = normMat[:numTestVecs,:]\n",
    "    testY = datingLabels[:numTestVecs]\n",
    "    # 3.在测试集上计算错误率\n",
    "    # 分类错误计数\n",
    "    errorCount = 0\n",
    "    for x,y in zip(testX,testY):\n",
    "        # 前numTestVecs个数据作为测试集，\n",
    "        # 后m-numTestVecs个数据作为训练集\n",
    "        # k选择label数+1（结果比较好）\n",
    "        classifierResult = classify0(\n",
    "            x, trainX, trainY, 4)\n",
    "        print(\"分类结果:%d\\t真实类别:%d\" % (\n",
    "            classifierResult, y))\n",
    "        if classifierResult != y:\n",
    "            errorCount += 1\n",
    "    print(\"错误率:%.2f%%\" % (errorCount/numTestVecs*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试数据并评价模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datingClassTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在线输入样本并预测分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyPerson():\n",
    "    \"\"\"\n",
    "    函数说明：通过输入一个人的三个特征，进行分类输出\n",
    "\n",
    "    Parameters:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # 输出结果\n",
    "    resultList = ['讨厌', '有些喜欢', '非常喜欢']\n",
    "    # 三维特征用户输入\n",
    "    percentTats = float(input(\"玩视频游戏所消耗时间百分比：\"))\n",
    "    ffMiles = float(input(\"每年获得的飞行常客里程数：\"))\n",
    "    iceCream = float(input(\"每周消费的冰淇淋公升数：\"))\n",
    "    # 打开的文件名\n",
    "    filename = \"datingTestSet.txt\"\n",
    "    # 打开并处理数据\n",
    "    datingDataMat, datingLabels = file2matrix(filename)\n",
    "    # 训练集归一化\n",
    "    normMat, ranges, minVals = autoNorm(datingDataMat)\n",
    "    # 生成NumPy数组，测试集\n",
    "    inArr = np.array([percentTats, ffMiles, iceCream])\n",
    "    # 测试集归一化\n",
    "    norminArr = (inArr - minVals) / ranges\n",
    "    # 返回分类结果\n",
    "    classifierResult = classify0(norminArr, \n",
    "                                 normMat, datingLabels, 4)\n",
    "    # 打印结果\n",
    "    print(\"你可能%s这个人\" % (\n",
    "        resultList[classifierResult - 1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifyPerson()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 交叉验证 寻找最佳的k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"准备好打乱的数据集\n",
    "return:\n",
    "    norm_mat特征矩阵\n",
    "    labels 标签数组\n",
    "\"\"\"\n",
    "def pre_data():\n",
    "    data_mat, labels = file2matrix('datingTestSet.txt')\n",
    "    norm_mat, min_vector, max_vector = autoNorm(data_mat)\n",
    "    n = norm_mat.shape[0]\n",
    "    idxs = np.random.permutation(n)\n",
    "    norm_mat = norm_mat[idxs]\n",
    "    labels = labels[idxs]\n",
    "    return norm_mat, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"交叉验证\n",
    "\n",
    "return：\n",
    "    平均错误率\n",
    "\"\"\"\n",
    "def cross_validation(norm_mat, labels, k):\n",
    "    # 交叉验证，如把样本分成10份\n",
    "    n = norm_mat.shape[0]\n",
    "    test_n = int(n * 0.1)\n",
    "    error_list = []\n",
    "    for i in range(10):\n",
    "        #print(\"i=\", i)\n",
    "        test_start = test_n * i\n",
    "        test_end = test_n * (i + 1)\n",
    "        # 测试集\n",
    "        testx = norm_mat[test_start:test_end,:]\n",
    "        testy = labels[test_start:test_end]\n",
    "        # 训练集\n",
    "        #trainx\n",
    "        trainx1 = norm_mat[:test_start,:]\n",
    "        trainx2 =  norm_mat[test_end:,:]\n",
    "        if(trainx1.shape[0] == 0):\n",
    "            trainx = trainx2\n",
    "        elif(trainx2.shape[0] == 0):\n",
    "            trainx = trainx1\n",
    "        else:\n",
    "            trainx = np.r_[trainx1, trainx2]\n",
    "        # trainy\n",
    "        trainy1 = labels[:test_start]\n",
    "        trainy2 = labels[test_end:]\n",
    "        if(trainy1.shape[0] == 0):\n",
    "            trainy = trainy2\n",
    "        elif(trainy2.shape[0] == 0):\n",
    "            trainy = trainy1\n",
    "        else:\n",
    "            trainy = np.r_[trainy1, trainy2]\n",
    "        # 计算一次错误率\n",
    "        error_count = 0\n",
    "        for x,y in zip(testx,testy):\n",
    "            xlabel = classify0(x, trainx, trainy, k)\n",
    "            if xlabel != y:\n",
    "                error_count += 1\n",
    "        error1 = error_count/test_n*100\n",
    "        # 将本次错误率保存到error_list\n",
    "        error_list.append(error1)\n",
    "    # 算平均错误率\n",
    "    err_u = np.array(error_list).sum(0) / len(error_list)\n",
    "    return err_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_mat, labels = pre_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation(norm_mat, labels, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "k_list = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,30,40,50,60,70,80,90,100]\n",
    "p_list = []\n",
    "for k in k_list:\n",
    "    p = cross_validation(norm_mat, labels, k)\n",
    "    p_list.append(p)\n",
    "# k_list = np.array(k_list)\n",
    "# p_list = np.array(p_list)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(k_list, p_list, c='red')\n",
    "ax.scatter(k_list, p_list, c='blue')\n",
    "plt.xlabel('k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
